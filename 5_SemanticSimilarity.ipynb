{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_SemanticSimilarity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sishef/nlpworkshop/blob/main/5_SemanticSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 5: Measuring semantic similarty \n",
        "(how similar is the meaning, not the spelling)"
      ],
      "metadata": {
        "id": "tuNDwDaBVtLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Simple string similarity doesn't understand meaning at all"
      ],
      "metadata": {
        "id": "ZTHLYJLfqGbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install levenshtein"
      ],
      "metadata": {
        "id": "YFGYjqOxWJ6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Levenshtein\n",
        "\n",
        "def remove_punctuation(msg):\n",
        "  symbols = ['?','-',',',':',';']\n",
        "  for symbol in symbols:\n",
        "    msg = msg.replace(symbol, '')\n",
        "  return msg\n",
        "\n",
        "def tokenize(msg):\n",
        "  msg = msg.lower()\n",
        "  tokens = msg.split(' ')\n",
        "  return tokens\n",
        "\n",
        "   \n",
        "sentence1 = \"The earth is round\"\n",
        "sentence2 = \"Our planet is a sphere\"\n",
        "sentence3 = \"Some cheese is orange\"\n",
        "\n",
        "clean_sentence1 = ''.join(tokenize(remove_punctuation(sentence1)))\n",
        "clean_sentence2 = ''.join(tokenize(remove_punctuation(sentence2)))\n",
        "clean_sentence3 = ''.join(tokenize(remove_punctuation(sentence3)))\n",
        "\n",
        "print(Levenshtein.distance(clean_sentence1, clean_sentence2))\n",
        "print(Levenshtein.distance(clean_sentence1, clean_sentence3))\n"
      ],
      "metadata": {
        "id": "H267kjrnV9_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using our distance measure from the last excercise we can see that sentence 2 and sentence 3 have very similar distances to sentence 1. In fact sentence 3 is a bit closer.\n",
        "\n",
        "Of course when we consider the meaning of these, sentence 2 is almost identical in meaning to sentence 1, whereas sentence 3 is totally different."
      ],
      "metadata": {
        "id": "kBuB1thIXAVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. The power of machine learning"
      ],
      "metadata": {
        "id": "Mx9H5JebXive"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to use some state of the art machine learning models to try and identify sentences that have similar sematics (meaning). First we have to install the libraries and create the model object (which takes a while to download as it is quite big)."
      ],
      "metadata": {
        "id": "9PQPMNonX0-9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTVD79zvSssq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('stsb-roberta-large')"
      ],
      "metadata": {
        "id": "U4NiO8sSTEhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets see what we can do.\n",
        "With this library we have to encode the sentence into 'embeddings', which is similar to our previous tokenization work except the embeddings are a list of special numbers derived from the words. Then we can use a mathematical similarity measure called 'cosine similarity' to see how similar the sentences are:"
      ],
      "metadata": {
        "id": "-zkPgE8nYMJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
        "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
        "embedding3 = model.encode(sentence3, convert_to_tensor=True)\n",
        "\n",
        "cosine_scores1_2 = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "cosine_scores1_3 = util.pytorch_cos_sim(embedding1, embedding3)\n",
        "\n",
        "print(f\"The similarity between '{sentence1}' and '{sentence2}' is {cosine_scores1_2.item()}\")\n",
        "print(f\"The similarity between '{sentence1}' and '{sentence3}' is {cosine_scores1_3.item()}\")"
      ],
      "metadata": {
        "id": "nbROFsm5UGGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Pl4EiLa7bdLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazing! The machine was able to understand that 'our planet' is similar in meaning to 'the earth', and 'is round' is similar in meaning to 'is a sphere'.\n",
        "\n",
        "**How does this magic work? --> side presentation**"
      ],
      "metadata": {
        "id": "9xd7rBTCZwu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK:** Use this new semantic similarity measure to identify better chatbot responses. Note, the model.enode(...) function call is a bit slow, so you should consider doing this once for each sentence when the training data is loaded into the chatbot rather than every time a user inputs new data.\n",
        "**TIP:** Take a look at Example_ChatterbotCorpusWithTransformer.ipynb for some help on making the up-front training fast\n",
        "\n",
        "If you have any extra time you can learn more about 'semantic search' here: https://sbert.net/examples/applications/semantic-search/README.html\n"
      ],
      "metadata": {
        "id": "cvLS6jIwoYT9"
      }
    }
  ]
}