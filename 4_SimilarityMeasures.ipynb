{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_SimilarityMeasures.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sishef/nlpworkshop/blob/main/4_SimilarityMeasures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 4: Measuring similarity of text (aka 'distance' between strings)"
      ],
      "metadata": {
        "id": "o8QkftMI3UP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want our conversation chatbot to be able to respond to a user based on it's training data. One way to do this is to find sentences in our training data that are similar to the input just provided by the user and simply return the subsequent sentence from the training data.\n",
        "\n",
        "How do we find similar statments? \n",
        "We can compare the user's input to each item in the training data and choose the closest match. This requires us to have a way to measure similarity of sentences, which is what we'll focus on here.\n",
        " "
      ],
      "metadata": {
        "id": "80TVMf2y3ycA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. String similarity\n",
        "We'll start with measuring the similarity of two strings, and then use that to compare the similarity of two sentences."
      ],
      "metadata": {
        "id": "7Ua0NYPV6PEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string1 = \"chatbot\"\n",
        "string2 = \"splatbot\"\n",
        "string3 = \"elephant\"\n",
        "print(f\"Is '{string1}' the same as '{string2}'? {string1 == string2}\")\n",
        "print(f\"Is '{string1}' the same as '{string3}'? {string1 == string3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA_tHQl-65fT",
        "outputId": "9790a8b7-3614-4b21-82f9-4abc05e0343a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is 'chatbot' the same as 'splatbot'? False\n",
            "Is 'chatbot' the same as 'elephant'? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this. Obviously all three words are different, so comparing them for equality returns False. However, you & I can see that 'splatbot' is much closer to 'chatbot' than 'elephant'. How could we turn that similarity into a number? \n",
        "Lets look at Levenshtein Distance (aka edit distance)..."
      ],
      "metadata": {
        "id": "uAhxa2qD7h7Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZCQe4sJ0kH8"
      },
      "outputs": [],
      "source": [
        "pip install levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import Levenshtein\n",
        "\n",
        "distance1_2 = Levenshtein.distance(string1, string2)\n",
        "print(f\"The Levenshtein distance between '{string1}' and '{string2}' is {distance1_2}\")\n",
        "\n",
        "distance1_3 = Levenshtein.distance(string1, string3)\n",
        "print(f\"The Levenshtein distance between '{string1}' and '{string3}' is {distance1_3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFa-qWOT0pdW",
        "outputId": "ef996da0-1331-4bc6-d294-186e348ed912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Levenshtein distance between 'chatbot' and 'splatbot' is 3\n",
            "The Levenshtein distance between 'chatbot' and 'elephant' is 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the pip install... section and then run this code. Great!\n",
        "\n",
        "**We now have a number which shows us that the 'distance' between 'chatbot' and 'splatbot' is lower than the distance between 'chatbot' and 'elephant'.**\n",
        "\n",
        "How is the Levenshtein distance actually calculated? Its pretty simple; it's just the total number of character insertions, deletions and substitutions you need to make to change one string into another. The number is 3 for 'splatbot' because you need to remove the first 's' and then switch the 'p' and 'l' for 'c' and 'h'.\n",
        "\n",
        "You can find more detail on wkipedia here : https://en.wikipedia.org/wiki/Levenshtein_distance\n"
      ],
      "metadata": {
        "id": "y-qDT-3u9ssd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Levenshtein distance isn't the only way to measure similarity:"
      ],
      "metadata": {
        "id": "_ZCuNYnpENXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity1_2 = Levenshtein.jaro_winkler(string1, string2)\n",
        "print(f\"The Jaro Winkler similarity between '{string1}' and '{string2}' is {similarity1_2}\")\n",
        "\n",
        "similarity1_3 = Levenshtein.jaro_winkler(string1, string3)\n",
        "print(f\"The Jaro Winkler similarity between '{string1}' and '{string3}' is {similarity1_3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6ilQ0zB9mpg",
        "outputId": "6105a43f-432a-4b60-a3af-457f0ec08a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Jaro Winkler similarity between 'chatbot' and 'splatbot' is 0.7797619047619048\n",
            "The Jaro Winkler similarity between 'chatbot' and 'elephant' is 0.6011904761904762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is using a different method called 'Jaro Winkler', which measures similarity (the inverse of difference) using a different algorithm.\n",
        "\n",
        "See here for details: https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance\n",
        "\n",
        "Note: This method gives a similarity score between 0 and 1. To convert to a distance measure we can just subtract the result from 1"
      ],
      "metadata": {
        "id": "zYc5_ModEyKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. sentence similarity\n",
        "Lets use the measures above to compare sentences instead of single words"
      ],
      "metadata": {
        "id": "J-u4T96kGUwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(msg):\n",
        "  symbols = ['?','-',',',':',';']\n",
        "  for symbol in symbols:\n",
        "    msg = msg.replace(symbol, '')\n",
        "  return msg\n",
        "\n",
        "def tokenize(msg):\n",
        "  msg = msg.lower()\n",
        "  tokens = msg.split(' ')\n",
        "  return tokens\n",
        "\n",
        "sentence1 = tokenize(remove_punctuation('I think chatbots are the best'))\n",
        "sentence2 = tokenize(remove_punctuation('You think chatbots are cool'))\n",
        "sentence3 = tokenize(remove_punctuation('I eat fish for breakfast'))\n"
      ],
      "metadata": {
        "id": "VpuHBrfDGTOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know from the our previous work that its a good idea to clean up the statments and tokenize them before trying to compare them. Now that we have a list of workd (tokens) for each statment what shall we do?\n",
        "\n",
        "Simplest idea: recombine them and use the distance measure."
      ],
      "metadata": {
        "id": "C_ZPub_wH6m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sentence1 = ('').join(sentence1)\n",
        "clean_sentence2 = ('').join(sentence2)\n",
        "clean_sentence3 = ('').join(sentence3)\n",
        "\n",
        "distance1_2 = Levenshtein.distance(clean_sentence1, clean_sentence2)\n",
        "distance1_3 = Levenshtein.distance(clean_sentence1, clean_sentence3)\n",
        "                                                                      \n",
        "print(f\"The distance between {clean_sentence1} and {clean_sentence2} is {distance1_2}\")\n",
        "print(f\"The distance between {clean_sentence1} and {clean_sentence3} is {distance1_3}\")\n"
      ],
      "metadata": {
        "id": "Jk6ZT_F9IW9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and here we go - we have been able to detect the sentences that are most similar (smaller distance).\n",
        "\n",
        "**TASK:** Use this technique in the chatbot to find the sentence in the training data with shortest distance to the user input, and return the subsequent sentence\n",
        "\n",
        "**EXTRA TASK:** Try using Jaro Winkler instead. Which one do you think works best?"
      ],
      "metadata": {
        "id": "z8BhFCMtJFUz"
      }
    }
  ]
}