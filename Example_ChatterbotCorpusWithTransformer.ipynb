{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example_ChatterbotCorpusWithTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6GW7rOTuVhMdgXvyZ/RiQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sishef/nlpworkshop/blob/main/Example_ChatterbotCorpusWithTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Using semantic similarity on an existing corpus of sentences\n",
        "This example shows you how to efficiently install an exisating corpus of conversation data (in chatterbot-corpus) and then efficiently encode all of it into embeddings that can be used afterwards to quickly search for semantically similar sentences."
      ],
      "metadata": {
        "id": "Jvc8ZmeUFUT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** When you get to the code below that actually does the encoding of all the corpus sentences it is quite slow to run. You can darmatically speed it up by telling Colab to use GPUs (hardware acceleration).\n",
        "Do this by going to the Runtim menu option above --> Change Runtime Type --> Hardware Accelerator = GPU"
      ],
      "metadata": {
        "id": "AM1OC0nIEm94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP2j2RWtwJ-S"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "!pip install chatterbot-corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "#model = SentenceTransformer('stsb-roberta-base')\n",
        "model = SentenceTransformer('stsb-roberta-large') #--> the best one but slow to encode if you aren't using GPU"
      ],
      "metadata": {
        "id": "jvVc8XW2wO7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import inspect\n",
        "import torch\n",
        "from yaml import load\n",
        "import chatterbot_corpus\n",
        "\n",
        "# This is the location of the corpus YAML files installed with the chatterbot corpus package\n",
        "data_path = os.path.join(os.path.dirname(inspect.getfile(chatterbot_corpus)), 'data/english')\n",
        "\n",
        "all_data = []\n",
        "all_embeddings = []\n",
        "\n",
        "# Loop through each YAML file\n",
        "for conv_file in os.listdir(data_path):\n",
        "\n",
        "  # Read the YAML and turn into a long list of sentences  \n",
        "  conv_data = load(open(os.path.join(data_path, conv_file), 'r'))\n",
        "  conv_data = [item for sublist in conv_data['conversations'] for item in sublist]\n",
        "\n",
        "  # Use the transformer model to encode the full list of sentences and store them in our variable\n",
        "  print(f\"Encoding {len(conv_data)} lines in file {conv_file}...\")\n",
        "  all_embeddings.append(model.encode(conv_data, convert_to_tensor=True))\n",
        "  all_data.append(conv_data)\n",
        "\n",
        "# Turn a list of lists into one big list for both the embeddings and the original sentence data\n",
        "all_embeddings = torch.cat(all_embeddings)\n",
        "all_data = [item for sublist in all_data for item in sublist]"
      ],
      "metadata": {
        "id": "oNJJzS-bwrtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets try it out\n",
        "query=\"what is your favorite dish?\"\n",
        "\n",
        "query_emb = model.encode(query, convert_to_tensor=True)\n",
        "cos_scores = util.cos_sim(query_emb, all_embeddings)[0] # runs cosine similarity against every set of embeddings in the list (i.e. all our training data)\n",
        "top_results = torch.topk(cos_scores, 5) # pick the 5 best matches\n",
        "\n",
        "for score, idx in zip(top_results[0], top_results[1]):\n",
        "  print(f\"match: {all_data[idx]}, response: {all_data[idx+1]}, score: {score}\")"
      ],
      "metadata": {
        "id": "X_HQQ2nh32mq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}