{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_GeneratingTextWithML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM55RXDYtNWoenGP4qB30By",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sishef/nlpworkshop/blob/main/6_GeneratingTextWithML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 6: Using machine learning to generate responses\n",
        "In our previous chatbot we used ML to find sentences with similar meaning to those in our training data and then responded with the subsequent sentence in the data.\n",
        "This is ok but means the chatbot's responses are limited to the exact sentences we have trained it with. We can do better!\n",
        "\n",
        "DialogGPT is a state of the art machine learning text generator. It can actually create totally new sentences about most subjects that make sense to the conversation. It has been trained on 147 million conversations taken from Reddit, so most topics are covered.\n",
        "\n",
        "Here's some sample code you can run and then use in your chatbot. Don't worry too much about all the detail just yet."
      ],
      "metadata": {
        "id": "RxuDOajZN3hB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjgLapEJN0y_"
      },
      "outputs": [],
      "source": [
        "% pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"microsoft/DialoGPT-large\"\n",
        "#model_name = \"microsoft/DialoGPT-medium\"\n",
        "# model_name = \"microsoft/DialoGPT-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "-1QmA7p_Pevm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chatting 5 times with nucleus sampling & tweaking temperature\n",
        "for step in range(5):\n",
        "\n",
        "    # take user input\n",
        "    text = input(\">> You:\")\n",
        "\n",
        "    # encode the input and add end of string token\n",
        "    input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors=\"pt\")\n",
        "\n",
        "    # concatenate new user input with chat history (if there is)\n",
        "    bot_input_ids = torch.cat([chat_history_ids, input_ids], dim=-1) if step > 0 else input_ids\n",
        "\n",
        "    # generate a bot response\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        top_k=0,\n",
        "        temperature=0.75,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    #print the output\n",
        "    output = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    print(f\"DialoGPT: {output}\")"
      ],
      "metadata": {
        "id": "KCy2T1J-PvJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few things to notice about this code:\n",
        "\n",
        "\n",
        "\n",
        "1. There are small, medium and large versions of the model. The smallest downloads quickest and generates results fastest, but how does it output compare versus the medium and large?\n",
        "\n",
        "2. You already know about tokenization. This code uses a tokenizer from the transformers library to turn your sentence into tokens.\n",
        "\n",
        "3. The response is created by the 'model.generate(...)' call. This function takes in some text that has already been encoded (the bot_input_ids) and returns a response that is encoded. That's why we have to use 'tokenizer.decode(...)' to translate from these encodings back into regular text.\n",
        "\n",
        "4. Notice also that we are remembering all of the previous sentences and adding them to our new sentence when we pass them into the model. This means the model should use the full context of the conversation so far when generating the next response, as opposed to just reacting to the latest user input.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yqc-Eu2ZSUis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK:** Change your current chatbot to use the DialogGPT model to generate it's responses\n",
        "\n",
        "**TASK:** Play with the different models (small, medium, large) to see how much better or worse each one is\n",
        "\n",
        "**TASK:** Change the code to generate two responses, one that uses the current conversation history and one that does not. Do the responses make a lot more sense when using the history or not?"
      ],
      "metadata": {
        "id": "l_cl_GwTVH8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: This code is taken from this tutorial: https://www.thepythoncode.com/article/conversational-ai-chatbot-with-huggingface-transformers-in-python\n"
      ],
      "metadata": {
        "id": "HV2iyDdMWwpy"
      }
    }
  ]
}